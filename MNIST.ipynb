{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7bzt5DRDtVtK2rT6nLj8A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youhs1125/Kaggle/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv9T3hMGyQgy",
        "outputId": "1a186b9c-744c-47b6-cff3-1df02192d003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "key_path = '/content/drive/MyDrive/Kaggle/Kaggle_key/kaggle.json'\n",
        "\n",
        "with open(key_path) as f:\n",
        "  api = json.load(f)\n",
        "  os.environ[\"KAGGLE_USERNAME\"] = api['username']\n",
        "  os.environ[\"KAGGLE_KEY\"] = api['key']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c digit-recognizer\n",
        "!unzip -qq \"/content/digit-recognizer.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qLby_pFyYxm",
        "outputId": "8e49d99e-28e5-4dff-fa39-4d05b71bac3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading digit-recognizer.zip to /content\n",
            " 85% 13.0M/15.3M [00:01<00:00, 15.1MB/s]\n",
            "100% 15.3M/15.3M [00:01<00:00, 9.40MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.utils.data as td\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "gba-6ODsys2O"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjF-i297yw3V",
        "outputId": "87a99bec-e3b7-4899-e966-0294098aa3b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  loss_track = np.array([])\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    X = X.unsqueeze(dim = 1)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred,y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch%100 == 0:\n",
        "      loss, current = loss.item(),batch*len(X)\n",
        "      loss_track = np.append(loss_track,np.array(loss))\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "    \n",
        "  return np.mean(loss_track)\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0,0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X,y = X.to(device), y.to(device)\n",
        "      X = X.unsqueeze(dim = 1)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Avg loss: {test_loss:>8f}\\n\")\n",
        "  return test_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy8tgBIHyyil",
        "outputId": "d7d54de7-f1da-4c74-be0a-b8fab9843bb7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dsets.MNIST(root=\"\",train=True,transform=transforms.ToTensor(),download=True)\n",
        "\n",
        "dataset.data = dataset.data.type(torch.FloatTensor)/255.\n",
        "\n",
        "dataset = td.TensorDataset(dataset.data,dataset.targets)\n",
        "\n",
        "\n",
        "trainsize = int(len(dataset)*0.8)\n",
        "validsize = len(dataset) - trainsize\n",
        "batchsize = 64\n",
        "\n",
        "trainset, validset = td.random_split(dataset,[trainsize,validsize])\n",
        "\n",
        "train_dataloader = td.DataLoader(trainset, batch_size=batchsize, shuffle=True)\n",
        "valid_dataloader = td.DataLoader(validset, batch_size=batchsize, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Wub4Y0zKOV",
        "outputId": "93a243b6-3ec9-4b95-8ed4-769429a8215d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Conv2d(1,64,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "        nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128,128,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "        nn.Conv2d(128,256,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(256,512,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512,512,kernel_size=3,padding=1),\n",
        "        nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(4608,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,10),\n",
        "        nn.Softmax()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)"
      ],
      "metadata": {
        "id": "yXORFE5HzNYF"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 80\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train_loss = np.empty\n",
        "val_loss = []\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n----------------\")\n",
        "  loss_train = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  train_loss = np.append(train_loss,loss_train)\n",
        "  loss_test = test_loop(valid_dataloader, model, loss_fn)\n",
        "  val_loss.append(loss_test)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_QMeDPN2dKO",
        "outputId": "9ff28bd1-174e-4049-c87d-ad443a6b20b5"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "----------------\n",
            "loss: 1.960298 [    0/48000]\n",
            "loss: 1.759228 [ 6400/48000]\n",
            "loss: 1.774644 [12800/48000]\n",
            "loss: 1.741794 [19200/48000]\n",
            "loss: 1.724216 [25600/48000]\n",
            "loss: 1.719120 [32000/48000]\n",
            "loss: 1.792878 [38400/48000]\n",
            "loss: 1.727192 [44800/48000]\n",
            "Avg loss: 1.724465\n",
            "\n",
            "Epoch 2\n",
            "----------------\n",
            "loss: 1.832512 [    0/48000]\n",
            "loss: 1.670689 [ 6400/48000]\n",
            "loss: 1.745089 [12800/48000]\n",
            "loss: 1.662345 [19200/48000]\n",
            "loss: 1.700882 [25600/48000]\n",
            "loss: 1.649306 [32000/48000]\n",
            "loss: 1.729638 [38400/48000]\n",
            "loss: 1.628545 [44800/48000]\n",
            "Avg loss: 1.638880\n",
            "\n",
            "Epoch 3\n",
            "----------------\n",
            "loss: 1.640861 [    0/48000]\n",
            "loss: 1.693037 [ 6400/48000]\n",
            "loss: 1.624962 [12800/48000]\n",
            "loss: 1.680737 [19200/48000]\n",
            "loss: 1.620701 [25600/48000]\n",
            "loss: 1.505651 [32000/48000]\n",
            "loss: 1.555347 [38400/48000]\n",
            "loss: 1.572353 [44800/48000]\n",
            "Avg loss: 1.613298\n",
            "\n",
            "Epoch 4\n",
            "----------------\n",
            "loss: 1.607491 [    0/48000]\n",
            "loss: 1.631871 [ 6400/48000]\n",
            "loss: 1.633008 [12800/48000]\n",
            "loss: 1.583043 [19200/48000]\n",
            "loss: 1.658757 [25600/48000]\n",
            "loss: 1.561501 [32000/48000]\n",
            "loss: 1.512502 [38400/48000]\n",
            "loss: 1.479758 [44800/48000]\n",
            "Avg loss: 1.528710\n",
            "\n",
            "Epoch 5\n",
            "----------------\n",
            "loss: 1.503228 [    0/48000]\n",
            "loss: 1.548295 [ 6400/48000]\n",
            "loss: 1.537587 [12800/48000]\n",
            "loss: 1.512453 [19200/48000]\n",
            "loss: 1.500059 [25600/48000]\n",
            "loss: 1.520835 [32000/48000]\n",
            "loss: 1.522047 [38400/48000]\n",
            "loss: 1.500129 [44800/48000]\n",
            "Avg loss: 1.512801\n",
            "\n",
            "Epoch 6\n",
            "----------------\n",
            "loss: 1.497357 [    0/48000]\n",
            "loss: 1.535039 [ 6400/48000]\n",
            "loss: 1.530987 [12800/48000]\n",
            "loss: 1.511046 [19200/48000]\n",
            "loss: 1.494451 [25600/48000]\n",
            "loss: 1.465460 [32000/48000]\n",
            "loss: 1.492910 [38400/48000]\n",
            "loss: 1.492146 [44800/48000]\n",
            "Avg loss: 1.508763\n",
            "\n",
            "Epoch 7\n",
            "----------------\n",
            "loss: 1.498379 [    0/48000]\n",
            "loss: 1.483500 [ 6400/48000]\n",
            "loss: 1.463496 [12800/48000]\n",
            "loss: 1.516476 [19200/48000]\n",
            "loss: 1.499543 [25600/48000]\n",
            "loss: 1.511341 [32000/48000]\n",
            "loss: 1.509485 [38400/48000]\n",
            "loss: 1.484298 [44800/48000]\n",
            "Avg loss: 1.501504\n",
            "\n",
            "Epoch 8\n",
            "----------------\n",
            "loss: 1.520310 [    0/48000]\n",
            "loss: 1.507435 [ 6400/48000]\n",
            "loss: 1.503692 [12800/48000]\n",
            "loss: 1.475091 [19200/48000]\n",
            "loss: 1.537426 [25600/48000]\n",
            "loss: 1.493233 [32000/48000]\n",
            "loss: 1.498258 [38400/48000]\n",
            "loss: 1.518185 [44800/48000]\n",
            "Avg loss: 1.503423\n",
            "\n",
            "Epoch 9\n",
            "----------------\n",
            "loss: 1.489417 [    0/48000]\n",
            "loss: 1.489885 [ 6400/48000]\n",
            "loss: 1.495813 [12800/48000]\n",
            "loss: 1.499131 [19200/48000]\n",
            "loss: 1.467711 [25600/48000]\n",
            "loss: 1.503931 [32000/48000]\n",
            "loss: 1.482607 [38400/48000]\n",
            "loss: 1.462094 [44800/48000]\n",
            "Avg loss: 1.490946\n",
            "\n",
            "Epoch 10\n",
            "----------------\n",
            "loss: 1.465627 [    0/48000]\n",
            "loss: 1.507990 [ 6400/48000]\n",
            "loss: 1.494215 [12800/48000]\n",
            "loss: 1.484592 [19200/48000]\n",
            "loss: 1.495681 [25600/48000]\n",
            "loss: 1.480696 [32000/48000]\n",
            "loss: 1.477432 [38400/48000]\n",
            "loss: 1.479966 [44800/48000]\n",
            "Avg loss: 1.492828\n",
            "\n",
            "Epoch 11\n",
            "----------------\n",
            "loss: 1.506286 [    0/48000]\n",
            "loss: 1.462181 [ 6400/48000]\n",
            "loss: 1.504667 [12800/48000]\n",
            "loss: 1.498723 [19200/48000]\n",
            "loss: 1.461904 [25600/48000]\n",
            "loss: 1.461358 [32000/48000]\n",
            "loss: 1.462151 [38400/48000]\n",
            "loss: 1.494557 [44800/48000]\n",
            "Avg loss: 1.490152\n",
            "\n",
            "Epoch 12\n",
            "----------------\n",
            "loss: 1.469689 [    0/48000]\n",
            "loss: 1.478219 [ 6400/48000]\n",
            "loss: 1.497600 [12800/48000]\n",
            "loss: 1.501265 [19200/48000]\n",
            "loss: 1.529612 [25600/48000]\n",
            "loss: 1.483096 [32000/48000]\n",
            "loss: 1.483763 [38400/48000]\n",
            "loss: 1.489763 [44800/48000]\n",
            "Avg loss: 1.490628\n",
            "\n",
            "Epoch 13\n",
            "----------------\n",
            "loss: 1.500790 [    0/48000]\n",
            "loss: 1.462751 [ 6400/48000]\n",
            "loss: 1.504918 [12800/48000]\n",
            "loss: 1.510940 [19200/48000]\n",
            "loss: 1.465970 [25600/48000]\n",
            "loss: 1.478313 [32000/48000]\n",
            "loss: 1.476556 [38400/48000]\n",
            "loss: 1.497260 [44800/48000]\n",
            "Avg loss: 1.485720\n",
            "\n",
            "Epoch 14\n",
            "----------------\n",
            "loss: 1.463968 [    0/48000]\n",
            "loss: 1.508886 [ 6400/48000]\n",
            "loss: 1.465264 [12800/48000]\n",
            "loss: 1.491576 [19200/48000]\n",
            "loss: 1.480273 [25600/48000]\n",
            "loss: 1.493217 [32000/48000]\n",
            "loss: 1.514459 [38400/48000]\n",
            "loss: 1.463026 [44800/48000]\n",
            "Avg loss: 1.485339\n",
            "\n",
            "Epoch 15\n",
            "----------------\n",
            "loss: 1.468721 [    0/48000]\n",
            "loss: 1.475261 [ 6400/48000]\n",
            "loss: 1.462388 [12800/48000]\n",
            "loss: 1.480194 [19200/48000]\n",
            "loss: 1.462300 [25600/48000]\n",
            "loss: 1.462087 [32000/48000]\n",
            "loss: 1.479961 [38400/48000]\n",
            "loss: 1.471976 [44800/48000]\n",
            "Avg loss: 1.484919\n",
            "\n",
            "Epoch 16\n",
            "----------------\n",
            "loss: 1.484790 [    0/48000]\n",
            "loss: 1.467733 [ 6400/48000]\n",
            "loss: 1.461234 [12800/48000]\n",
            "loss: 1.480120 [19200/48000]\n",
            "loss: 1.463753 [25600/48000]\n",
            "loss: 1.482154 [32000/48000]\n",
            "loss: 1.461204 [38400/48000]\n",
            "loss: 1.461291 [44800/48000]\n",
            "Avg loss: 1.484394\n",
            "\n",
            "Epoch 17\n",
            "----------------\n",
            "loss: 1.461711 [    0/48000]\n",
            "loss: 1.492470 [ 6400/48000]\n",
            "loss: 1.488236 [12800/48000]\n",
            "loss: 1.475151 [19200/48000]\n",
            "loss: 1.496549 [25600/48000]\n",
            "loss: 1.465355 [32000/48000]\n",
            "loss: 1.483470 [38400/48000]\n",
            "loss: 1.498059 [44800/48000]\n",
            "Avg loss: 1.482998\n",
            "\n",
            "Epoch 18\n",
            "----------------\n",
            "loss: 1.465671 [    0/48000]\n",
            "loss: 1.461858 [ 6400/48000]\n",
            "loss: 1.465761 [12800/48000]\n",
            "loss: 1.495331 [19200/48000]\n",
            "loss: 1.491180 [25600/48000]\n",
            "loss: 1.463504 [32000/48000]\n",
            "loss: 1.507282 [38400/48000]\n",
            "loss: 1.473772 [44800/48000]\n",
            "Avg loss: 1.486467\n",
            "\n",
            "Epoch 19\n",
            "----------------\n",
            "loss: 1.468839 [    0/48000]\n",
            "loss: 1.476853 [ 6400/48000]\n",
            "loss: 1.476549 [12800/48000]\n",
            "loss: 1.465251 [19200/48000]\n",
            "loss: 1.487281 [25600/48000]\n",
            "loss: 1.467753 [32000/48000]\n",
            "loss: 1.477064 [38400/48000]\n",
            "loss: 1.480057 [44800/48000]\n",
            "Avg loss: 1.481785\n",
            "\n",
            "Epoch 20\n",
            "----------------\n",
            "loss: 1.476607 [    0/48000]\n",
            "loss: 1.476783 [ 6400/48000]\n",
            "loss: 1.461818 [12800/48000]\n",
            "loss: 1.502139 [19200/48000]\n",
            "loss: 1.472256 [25600/48000]\n",
            "loss: 1.461662 [32000/48000]\n",
            "loss: 1.472849 [38400/48000]\n",
            "loss: 1.461426 [44800/48000]\n",
            "Avg loss: 1.482406\n",
            "\n",
            "Epoch 21\n",
            "----------------\n",
            "loss: 1.461408 [    0/48000]\n",
            "loss: 1.461203 [ 6400/48000]\n",
            "loss: 1.461233 [12800/48000]\n",
            "loss: 1.481374 [19200/48000]\n",
            "loss: 1.477343 [25600/48000]\n",
            "loss: 1.474869 [32000/48000]\n",
            "loss: 1.493168 [38400/48000]\n",
            "loss: 1.477542 [44800/48000]\n",
            "Avg loss: 1.481204\n",
            "\n",
            "Epoch 22\n",
            "----------------\n",
            "loss: 1.477621 [    0/48000]\n",
            "loss: 1.475319 [ 6400/48000]\n",
            "loss: 1.479096 [12800/48000]\n",
            "loss: 1.463235 [19200/48000]\n",
            "loss: 1.461274 [25600/48000]\n",
            "loss: 1.477245 [32000/48000]\n",
            "loss: 1.469895 [38400/48000]\n",
            "loss: 1.481516 [44800/48000]\n",
            "Avg loss: 1.479358\n",
            "\n",
            "Epoch 23\n",
            "----------------\n",
            "loss: 1.477004 [    0/48000]\n",
            "loss: 1.461523 [ 6400/48000]\n",
            "loss: 1.469035 [12800/48000]\n",
            "loss: 1.479580 [19200/48000]\n",
            "loss: 1.461620 [25600/48000]\n",
            "loss: 1.461211 [32000/48000]\n",
            "loss: 1.465251 [38400/48000]\n",
            "loss: 1.477632 [44800/48000]\n",
            "Avg loss: 1.479892\n",
            "\n",
            "Epoch 24\n",
            "----------------\n",
            "loss: 1.461452 [    0/48000]\n",
            "loss: 1.476771 [ 6400/48000]\n",
            "loss: 1.463694 [12800/48000]\n",
            "loss: 1.461169 [19200/48000]\n",
            "loss: 1.461246 [25600/48000]\n",
            "loss: 1.465259 [32000/48000]\n",
            "loss: 1.461607 [38400/48000]\n",
            "loss: 1.479148 [44800/48000]\n",
            "Avg loss: 1.481328\n",
            "\n",
            "Epoch 25\n",
            "----------------\n",
            "loss: 1.484234 [    0/48000]\n",
            "loss: 1.478298 [ 6400/48000]\n",
            "loss: 1.461329 [12800/48000]\n",
            "loss: 1.461428 [19200/48000]\n",
            "loss: 1.492413 [25600/48000]\n",
            "loss: 1.465185 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.523852 [44800/48000]\n",
            "Avg loss: 1.481047\n",
            "\n",
            "Epoch 26\n",
            "----------------\n",
            "loss: 1.463707 [    0/48000]\n",
            "loss: 1.461153 [ 6400/48000]\n",
            "loss: 1.507518 [12800/48000]\n",
            "loss: 1.473514 [19200/48000]\n",
            "loss: 1.461203 [25600/48000]\n",
            "loss: 1.476732 [32000/48000]\n",
            "loss: 1.461166 [38400/48000]\n",
            "loss: 1.478687 [44800/48000]\n",
            "Avg loss: 1.480373\n",
            "\n",
            "Epoch 27\n",
            "----------------\n",
            "loss: 1.476664 [    0/48000]\n",
            "loss: 1.466008 [ 6400/48000]\n",
            "loss: 1.476781 [12800/48000]\n",
            "loss: 1.492939 [19200/48000]\n",
            "loss: 1.461209 [25600/48000]\n",
            "loss: 1.464617 [32000/48000]\n",
            "loss: 1.461152 [38400/48000]\n",
            "loss: 1.461387 [44800/48000]\n",
            "Avg loss: 1.478850\n",
            "\n",
            "Epoch 28\n",
            "----------------\n",
            "loss: 1.463653 [    0/48000]\n",
            "loss: 1.462440 [ 6400/48000]\n",
            "loss: 1.488099 [12800/48000]\n",
            "loss: 1.477034 [19200/48000]\n",
            "loss: 1.461651 [25600/48000]\n",
            "loss: 1.477194 [32000/48000]\n",
            "loss: 1.461187 [38400/48000]\n",
            "loss: 1.461628 [44800/48000]\n",
            "Avg loss: 1.477437\n",
            "\n",
            "Epoch 29\n",
            "----------------\n",
            "loss: 1.461453 [    0/48000]\n",
            "loss: 1.477029 [ 6400/48000]\n",
            "loss: 1.461153 [12800/48000]\n",
            "loss: 1.478838 [19200/48000]\n",
            "loss: 1.461305 [25600/48000]\n",
            "loss: 1.468296 [32000/48000]\n",
            "loss: 1.471252 [38400/48000]\n",
            "loss: 1.462194 [44800/48000]\n",
            "Avg loss: 1.479439\n",
            "\n",
            "Epoch 30\n",
            "----------------\n",
            "loss: 1.461171 [    0/48000]\n",
            "loss: 1.487127 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.477460 [19200/48000]\n",
            "loss: 1.461564 [25600/48000]\n",
            "loss: 1.461340 [32000/48000]\n",
            "loss: 1.464119 [38400/48000]\n",
            "loss: 1.476690 [44800/48000]\n",
            "Avg loss: 1.477942\n",
            "\n",
            "Epoch 31\n",
            "----------------\n",
            "loss: 1.461153 [    0/48000]\n",
            "loss: 1.476778 [ 6400/48000]\n",
            "loss: 1.461456 [12800/48000]\n",
            "loss: 1.463752 [19200/48000]\n",
            "loss: 1.461244 [25600/48000]\n",
            "loss: 1.463218 [32000/48000]\n",
            "loss: 1.477092 [38400/48000]\n",
            "loss: 1.464042 [44800/48000]\n",
            "Avg loss: 1.479485\n",
            "\n",
            "Epoch 32\n",
            "----------------\n",
            "loss: 1.465337 [    0/48000]\n",
            "loss: 1.462699 [ 6400/48000]\n",
            "loss: 1.461160 [12800/48000]\n",
            "loss: 1.461373 [19200/48000]\n",
            "loss: 1.461193 [25600/48000]\n",
            "loss: 1.491539 [32000/48000]\n",
            "loss: 1.461428 [38400/48000]\n",
            "loss: 1.476126 [44800/48000]\n",
            "Avg loss: 1.477703\n",
            "\n",
            "Epoch 33\n",
            "----------------\n",
            "loss: 1.479322 [    0/48000]\n",
            "loss: 1.461607 [ 6400/48000]\n",
            "loss: 1.472636 [12800/48000]\n",
            "loss: 1.476834 [19200/48000]\n",
            "loss: 1.462337 [25600/48000]\n",
            "loss: 1.462689 [32000/48000]\n",
            "loss: 1.461158 [38400/48000]\n",
            "loss: 1.462890 [44800/48000]\n",
            "Avg loss: 1.478422\n",
            "\n",
            "Epoch 34\n",
            "----------------\n",
            "loss: 1.476778 [    0/48000]\n",
            "loss: 1.461220 [ 6400/48000]\n",
            "loss: 1.476878 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.477534 [25600/48000]\n",
            "loss: 1.476833 [32000/48000]\n",
            "loss: 1.461154 [38400/48000]\n",
            "loss: 1.461155 [44800/48000]\n",
            "Avg loss: 1.476305\n",
            "\n",
            "Epoch 35\n",
            "----------------\n",
            "loss: 1.461717 [    0/48000]\n",
            "loss: 1.461153 [ 6400/48000]\n",
            "loss: 1.492395 [12800/48000]\n",
            "loss: 1.461154 [19200/48000]\n",
            "loss: 1.461194 [25600/48000]\n",
            "loss: 1.477576 [32000/48000]\n",
            "loss: 1.475736 [38400/48000]\n",
            "loss: 1.466466 [44800/48000]\n",
            "Avg loss: 1.477202\n",
            "\n",
            "Epoch 36\n",
            "----------------\n",
            "loss: 1.461579 [    0/48000]\n",
            "loss: 1.461246 [ 6400/48000]\n",
            "loss: 1.461191 [12800/48000]\n",
            "loss: 1.461156 [19200/48000]\n",
            "loss: 1.476771 [25600/48000]\n",
            "loss: 1.461151 [32000/48000]\n",
            "loss: 1.461263 [38400/48000]\n",
            "loss: 1.461182 [44800/48000]\n",
            "Avg loss: 1.475996\n",
            "\n",
            "Epoch 37\n",
            "----------------\n",
            "loss: 1.461197 [    0/48000]\n",
            "loss: 1.461168 [ 6400/48000]\n",
            "loss: 1.476776 [12800/48000]\n",
            "loss: 1.461153 [19200/48000]\n",
            "loss: 1.470412 [25600/48000]\n",
            "loss: 1.476493 [32000/48000]\n",
            "loss: 1.461152 [38400/48000]\n",
            "loss: 1.463450 [44800/48000]\n",
            "Avg loss: 1.474917\n",
            "\n",
            "Epoch 38\n",
            "----------------\n",
            "loss: 1.476793 [    0/48000]\n",
            "loss: 1.462961 [ 6400/48000]\n",
            "loss: 1.475916 [12800/48000]\n",
            "loss: 1.461194 [19200/48000]\n",
            "loss: 1.461154 [25600/48000]\n",
            "loss: 1.461165 [32000/48000]\n",
            "loss: 1.476755 [38400/48000]\n",
            "loss: 1.461172 [44800/48000]\n",
            "Avg loss: 1.475296\n",
            "\n",
            "Epoch 39\n",
            "----------------\n",
            "loss: 1.474631 [    0/48000]\n",
            "loss: 1.461526 [ 6400/48000]\n",
            "loss: 1.461203 [12800/48000]\n",
            "loss: 1.461620 [19200/48000]\n",
            "loss: 1.492355 [25600/48000]\n",
            "loss: 1.461196 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.476570 [44800/48000]\n",
            "Avg loss: 1.477676\n",
            "\n",
            "Epoch 40\n",
            "----------------\n",
            "loss: 1.476810 [    0/48000]\n",
            "loss: 1.461172 [ 6400/48000]\n",
            "loss: 1.478794 [12800/48000]\n",
            "loss: 1.461157 [19200/48000]\n",
            "loss: 1.505632 [25600/48000]\n",
            "loss: 1.477265 [32000/48000]\n",
            "loss: 1.461321 [38400/48000]\n",
            "loss: 1.461153 [44800/48000]\n",
            "Avg loss: 1.477900\n",
            "\n",
            "Epoch 41\n",
            "----------------\n",
            "loss: 1.462219 [    0/48000]\n",
            "loss: 1.461260 [ 6400/48000]\n",
            "loss: 1.470778 [12800/48000]\n",
            "loss: 1.461190 [19200/48000]\n",
            "loss: 1.461229 [25600/48000]\n",
            "loss: 1.461151 [32000/48000]\n",
            "loss: 1.476775 [38400/48000]\n",
            "loss: 1.461153 [44800/48000]\n",
            "Avg loss: 1.474796\n",
            "\n",
            "Epoch 42\n",
            "----------------\n",
            "loss: 1.461272 [    0/48000]\n",
            "loss: 1.461154 [ 6400/48000]\n",
            "loss: 1.461323 [12800/48000]\n",
            "loss: 1.461349 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.461175 [32000/48000]\n",
            "loss: 1.461355 [38400/48000]\n",
            "loss: 1.477012 [44800/48000]\n",
            "Avg loss: 1.475902\n",
            "\n",
            "Epoch 43\n",
            "----------------\n",
            "loss: 1.468860 [    0/48000]\n",
            "loss: 1.461162 [ 6400/48000]\n",
            "loss: 1.476743 [12800/48000]\n",
            "loss: 1.462169 [19200/48000]\n",
            "loss: 1.476741 [25600/48000]\n",
            "loss: 1.461167 [32000/48000]\n",
            "loss: 1.473103 [38400/48000]\n",
            "loss: 1.461159 [44800/48000]\n",
            "Avg loss: 1.476841\n",
            "\n",
            "Epoch 44\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.461166 [ 6400/48000]\n",
            "loss: 1.461172 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.462182 [25600/48000]\n",
            "loss: 1.476410 [32000/48000]\n",
            "loss: 1.461253 [38400/48000]\n",
            "loss: 1.461157 [44800/48000]\n",
            "Avg loss: 1.477662\n",
            "\n",
            "Epoch 45\n",
            "----------------\n",
            "loss: 1.461187 [    0/48000]\n",
            "loss: 1.461158 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461174 [25600/48000]\n",
            "loss: 1.476776 [32000/48000]\n",
            "loss: 1.461410 [38400/48000]\n",
            "loss: 1.462635 [44800/48000]\n",
            "Avg loss: 1.475527\n",
            "\n",
            "Epoch 46\n",
            "----------------\n",
            "loss: 1.476292 [    0/48000]\n",
            "loss: 1.476892 [ 6400/48000]\n",
            "loss: 1.463982 [12800/48000]\n",
            "loss: 1.463925 [19200/48000]\n",
            "loss: 1.465877 [25600/48000]\n",
            "loss: 1.483980 [32000/48000]\n",
            "loss: 1.461153 [38400/48000]\n",
            "loss: 1.508093 [44800/48000]\n",
            "Avg loss: 1.475213\n",
            "\n",
            "Epoch 47\n",
            "----------------\n",
            "loss: 1.461680 [    0/48000]\n",
            "loss: 1.461162 [ 6400/48000]\n",
            "loss: 1.461154 [12800/48000]\n",
            "loss: 1.476820 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.461152 [32000/48000]\n",
            "loss: 1.477006 [38400/48000]\n",
            "loss: 1.473805 [44800/48000]\n",
            "Avg loss: 1.474891\n",
            "\n",
            "Epoch 48\n",
            "----------------\n",
            "loss: 1.476775 [    0/48000]\n",
            "loss: 1.461151 [ 6400/48000]\n",
            "loss: 1.475948 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461334 [25600/48000]\n",
            "loss: 1.461171 [32000/48000]\n",
            "loss: 1.469843 [38400/48000]\n",
            "loss: 1.461151 [44800/48000]\n",
            "Avg loss: 1.475866\n",
            "\n",
            "Epoch 49\n",
            "----------------\n",
            "loss: 1.461154 [    0/48000]\n",
            "loss: 1.476777 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.462073 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.479147 [32000/48000]\n",
            "loss: 1.476490 [38400/48000]\n",
            "loss: 1.462195 [44800/48000]\n",
            "Avg loss: 1.476969\n",
            "\n",
            "Epoch 50\n",
            "----------------\n",
            "loss: 1.461966 [    0/48000]\n",
            "loss: 1.461383 [ 6400/48000]\n",
            "loss: 1.471800 [12800/48000]\n",
            "loss: 1.462134 [19200/48000]\n",
            "loss: 1.461323 [25600/48000]\n",
            "loss: 1.461151 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.461807 [44800/48000]\n",
            "Avg loss: 1.474318\n",
            "\n",
            "Epoch 51\n",
            "----------------\n",
            "loss: 1.461153 [    0/48000]\n",
            "loss: 1.461190 [ 6400/48000]\n",
            "loss: 1.461198 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.461436 [32000/48000]\n",
            "loss: 1.461788 [38400/48000]\n",
            "loss: 1.461291 [44800/48000]\n",
            "Avg loss: 1.475333\n",
            "\n",
            "Epoch 52\n",
            "----------------\n",
            "loss: 1.461190 [    0/48000]\n",
            "loss: 1.461151 [ 6400/48000]\n",
            "loss: 1.461173 [12800/48000]\n",
            "loss: 1.461172 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.476803 [32000/48000]\n",
            "loss: 1.461177 [38400/48000]\n",
            "loss: 1.461151 [44800/48000]\n",
            "Avg loss: 1.476124\n",
            "\n",
            "Epoch 53\n",
            "----------------\n",
            "loss: 1.476776 [    0/48000]\n",
            "loss: 1.492401 [ 6400/48000]\n",
            "loss: 1.461154 [12800/48000]\n",
            "loss: 1.461359 [19200/48000]\n",
            "loss: 1.461179 [25600/48000]\n",
            "loss: 1.461151 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.461593 [44800/48000]\n",
            "Avg loss: 1.474683\n",
            "\n",
            "Epoch 54\n",
            "----------------\n",
            "loss: 1.476659 [    0/48000]\n",
            "loss: 1.461272 [ 6400/48000]\n",
            "loss: 1.461347 [12800/48000]\n",
            "loss: 1.461251 [19200/48000]\n",
            "loss: 1.461200 [25600/48000]\n",
            "loss: 1.462673 [32000/48000]\n",
            "loss: 1.461224 [38400/48000]\n",
            "loss: 1.478964 [44800/48000]\n",
            "Avg loss: 1.475550\n",
            "\n",
            "Epoch 55\n",
            "----------------\n",
            "loss: 1.461186 [    0/48000]\n",
            "loss: 1.461151 [ 6400/48000]\n",
            "loss: 1.461155 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.461172 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.480498 [44800/48000]\n",
            "Avg loss: 1.475584\n",
            "\n",
            "Epoch 56\n",
            "----------------\n",
            "loss: 1.461186 [    0/48000]\n",
            "loss: 1.461867 [ 6400/48000]\n",
            "loss: 1.476776 [12800/48000]\n",
            "loss: 1.461360 [19200/48000]\n",
            "loss: 1.476770 [25600/48000]\n",
            "loss: 1.464293 [32000/48000]\n",
            "loss: 1.461169 [38400/48000]\n",
            "loss: 1.476808 [44800/48000]\n",
            "Avg loss: 1.473972\n",
            "\n",
            "Epoch 57\n",
            "----------------\n",
            "loss: 1.461154 [    0/48000]\n",
            "loss: 1.461246 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.491867 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.476776 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.461151 [44800/48000]\n",
            "Avg loss: 1.475263\n",
            "\n",
            "Epoch 58\n",
            "----------------\n",
            "loss: 1.492407 [    0/48000]\n",
            "loss: 1.461151 [ 6400/48000]\n",
            "loss: 1.477184 [12800/48000]\n",
            "loss: 1.461154 [19200/48000]\n",
            "loss: 1.476750 [25600/48000]\n",
            "loss: 1.461156 [32000/48000]\n",
            "loss: 1.463172 [38400/48000]\n",
            "loss: 1.461151 [44800/48000]\n",
            "Avg loss: 1.475611\n",
            "\n",
            "Epoch 59\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.461154 [ 6400/48000]\n",
            "loss: 1.461178 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461183 [25600/48000]\n",
            "loss: 1.476666 [32000/48000]\n",
            "loss: 1.461351 [38400/48000]\n",
            "loss: 1.461152 [44800/48000]\n",
            "Avg loss: 1.474217\n",
            "\n",
            "Epoch 60\n",
            "----------------\n",
            "loss: 1.461154 [    0/48000]\n",
            "loss: 1.461151 [ 6400/48000]\n",
            "loss: 1.461172 [12800/48000]\n",
            "loss: 1.461192 [19200/48000]\n",
            "loss: 1.476776 [25600/48000]\n",
            "loss: 1.476788 [32000/48000]\n",
            "loss: 1.463710 [38400/48000]\n",
            "loss: 1.492400 [44800/48000]\n",
            "Avg loss: 1.474735\n",
            "\n",
            "Epoch 61\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.461151 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.464618 [25600/48000]\n",
            "loss: 1.461151 [32000/48000]\n",
            "loss: 1.476776 [38400/48000]\n",
            "loss: 1.461152 [44800/48000]\n",
            "Avg loss: 1.474469\n",
            "\n",
            "Epoch 62\n",
            "----------------\n",
            "loss: 1.461165 [    0/48000]\n",
            "loss: 1.473648 [ 6400/48000]\n",
            "loss: 1.467090 [12800/48000]\n",
            "loss: 1.461169 [19200/48000]\n",
            "loss: 1.461157 [25600/48000]\n",
            "loss: 1.461155 [32000/48000]\n",
            "loss: 1.462653 [38400/48000]\n",
            "loss: 1.461808 [44800/48000]\n",
            "Avg loss: 1.474490\n",
            "\n",
            "Epoch 63\n",
            "----------------\n",
            "loss: 1.461241 [    0/48000]\n",
            "loss: 1.461151 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461158 [19200/48000]\n",
            "loss: 1.461209 [25600/48000]\n",
            "loss: 1.461152 [32000/48000]\n",
            "loss: 1.461190 [38400/48000]\n",
            "loss: 1.461152 [44800/48000]\n",
            "Avg loss: 1.474374\n",
            "\n",
            "Epoch 64\n",
            "----------------\n",
            "loss: 1.461437 [    0/48000]\n",
            "loss: 1.508389 [ 6400/48000]\n",
            "loss: 1.465485 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.463599 [25600/48000]\n",
            "loss: 1.461609 [32000/48000]\n",
            "loss: 1.476784 [38400/48000]\n",
            "loss: 1.463174 [44800/48000]\n",
            "Avg loss: 1.473334\n",
            "\n",
            "Epoch 65\n",
            "----------------\n",
            "loss: 1.461163 [    0/48000]\n",
            "loss: 1.461158 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461633 [25600/48000]\n",
            "loss: 1.461151 [32000/48000]\n",
            "loss: 1.461297 [38400/48000]\n",
            "loss: 1.461165 [44800/48000]\n",
            "Avg loss: 1.476385\n",
            "\n",
            "Epoch 66\n",
            "----------------\n",
            "loss: 1.461160 [    0/48000]\n",
            "loss: 1.461169 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461279 [19200/48000]\n",
            "loss: 1.461178 [25600/48000]\n",
            "loss: 1.461153 [32000/48000]\n",
            "loss: 1.461159 [38400/48000]\n",
            "loss: 1.461151 [44800/48000]\n",
            "Avg loss: 1.473287\n",
            "\n",
            "Epoch 67\n",
            "----------------\n",
            "loss: 1.461152 [    0/48000]\n",
            "loss: 1.461805 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461527 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.461210 [32000/48000]\n",
            "loss: 1.461156 [38400/48000]\n",
            "loss: 1.461151 [44800/48000]\n",
            "Avg loss: 1.474673\n",
            "\n",
            "Epoch 68\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.461157 [ 6400/48000]\n",
            "loss: 1.461152 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461152 [25600/48000]\n",
            "loss: 1.476693 [32000/48000]\n",
            "loss: 1.461531 [38400/48000]\n",
            "loss: 1.462465 [44800/48000]\n",
            "Avg loss: 1.474149\n",
            "\n",
            "Epoch 69\n",
            "----------------\n",
            "loss: 1.476776 [    0/48000]\n",
            "loss: 1.461217 [ 6400/48000]\n",
            "loss: 1.461152 [12800/48000]\n",
            "loss: 1.461152 [19200/48000]\n",
            "loss: 1.461165 [25600/48000]\n",
            "loss: 1.521355 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.461151 [44800/48000]\n",
            "Avg loss: 1.474339\n",
            "\n",
            "Epoch 70\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.476776 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461167 [25600/48000]\n",
            "loss: 1.461158 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.476417 [44800/48000]\n",
            "Avg loss: 1.474959\n",
            "\n",
            "Epoch 71\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.461154 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461830 [19200/48000]\n",
            "loss: 1.461160 [25600/48000]\n",
            "loss: 1.461221 [32000/48000]\n",
            "loss: 1.462481 [38400/48000]\n",
            "loss: 1.461152 [44800/48000]\n",
            "Avg loss: 1.473494\n",
            "\n",
            "Epoch 72\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.477463 [ 6400/48000]\n",
            "loss: 1.476776 [12800/48000]\n",
            "loss: 1.470520 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.476776 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.461154 [44800/48000]\n",
            "Avg loss: 1.473900\n",
            "\n",
            "Epoch 73\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.461152 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461162 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.461156 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.476780 [44800/48000]\n",
            "Avg loss: 1.475572\n",
            "\n",
            "Epoch 74\n",
            "----------------\n",
            "loss: 1.461154 [    0/48000]\n",
            "loss: 1.461152 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.461151 [32000/48000]\n",
            "loss: 1.461415 [38400/48000]\n",
            "loss: 1.461451 [44800/48000]\n",
            "Avg loss: 1.474418\n",
            "\n",
            "Epoch 75\n",
            "----------------\n",
            "loss: 1.461162 [    0/48000]\n",
            "loss: 1.461312 [ 6400/48000]\n",
            "loss: 1.462853 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.476750 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.461460 [44800/48000]\n",
            "Avg loss: 1.473794\n",
            "\n",
            "Epoch 76\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.461151 [ 6400/48000]\n",
            "loss: 1.476778 [12800/48000]\n",
            "loss: 1.461152 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.461151 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.461172 [44800/48000]\n",
            "Avg loss: 1.473185\n",
            "\n",
            "Epoch 77\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.461154 [ 6400/48000]\n",
            "loss: 1.476776 [12800/48000]\n",
            "loss: 1.461165 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.461354 [32000/48000]\n",
            "loss: 1.461152 [38400/48000]\n",
            "loss: 1.461151 [44800/48000]\n",
            "Avg loss: 1.475803\n",
            "\n",
            "Epoch 78\n",
            "----------------\n",
            "loss: 1.476753 [    0/48000]\n",
            "loss: 1.461151 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.477263 [19200/48000]\n",
            "loss: 1.478109 [25600/48000]\n",
            "loss: 1.477206 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.461152 [44800/48000]\n",
            "Avg loss: 1.474510\n",
            "\n",
            "Epoch 79\n",
            "----------------\n",
            "loss: 1.461184 [    0/48000]\n",
            "loss: 1.461151 [ 6400/48000]\n",
            "loss: 1.461151 [12800/48000]\n",
            "loss: 1.461159 [19200/48000]\n",
            "loss: 1.461156 [25600/48000]\n",
            "loss: 1.476776 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.461151 [44800/48000]\n",
            "Avg loss: 1.472505\n",
            "\n",
            "Epoch 80\n",
            "----------------\n",
            "loss: 1.461151 [    0/48000]\n",
            "loss: 1.461153 [ 6400/48000]\n",
            "loss: 1.461152 [12800/48000]\n",
            "loss: 1.461151 [19200/48000]\n",
            "loss: 1.461151 [25600/48000]\n",
            "loss: 1.461152 [32000/48000]\n",
            "loss: 1.461151 [38400/48000]\n",
            "loss: 1.461151 [44800/48000]\n",
            "Avg loss: 1.472956\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"test.csv\")\n",
        "sub = pd.read_csv(\"sample_submission.csv\")\n",
        "test = torch.FloatTensor(test.to_numpy())"
      ],
      "metadata": {
        "id": "FrzQ68Tt2oGE"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.reshape(-1, 1, 28, 28).to(device)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t302uVtnAG19",
        "outputId": "497506fd-bc14-44eb-b709-1f51be3a4e9b"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([28000, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  pred = model(test)\n",
        "  ans = torch.argmax(pred,dim=1)\n",
        "\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szeJCV2k_3oM",
        "outputId": "9421b524-39d9-4f91-9be5-100a13342081"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 0, 9,  ..., 3, 9, 2], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub['Label'] = ans.cpu()\n",
        "print(sub)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D6z10XuBIgE",
        "outputId": "332d7f43-7d31-4093-9813-b64a87f517ba"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       ImageId  Label\n",
            "0            1      2\n",
            "1            2      0\n",
            "2            3      9\n",
            "3            4      0\n",
            "4            5      3\n",
            "...        ...    ...\n",
            "27995    27996      9\n",
            "27996    27997      7\n",
            "27997    27998      3\n",
            "27998    27999      9\n",
            "27999    28000      2\n",
            "\n",
            "[28000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub.to_csv(\"predict.csv\",index=False)"
      ],
      "metadata": {
        "id": "jWdRwRuhB6Ym"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c digit-recognizer -f predict.csv -m \"AUTO_SUBMIT\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA9ee-ZpBYHs",
        "outputId": "1b1d375a-9298-4c0d-e26e-ba71e8317502"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 208k/208k [00:03<00:00, 69.2kB/s]\n",
            "Successfully submitted to Digit Recognizer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jzp2q4MtBszk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}